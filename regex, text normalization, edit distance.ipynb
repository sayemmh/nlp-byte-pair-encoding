{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regular Expressions, Text Normalization, Edit Distance\n",
    "\n",
    "**text normalization** - converting text to a more convenient, standard form\n",
    "\n",
    "**lemmatization** - the task of determining that two words have the same root, despite their surface differences\n",
    "- sang, sung, and sings are forms of the verb sing\n",
    "- lemmatizer (a function) maps these words to their lemma, sing\n",
    "\n",
    "**sentence segmentation** - breaking up a text into individual sentences, using cues like periods or exclamation points\n",
    "\n",
    "**edit distance** - metric that measures how similar two strings are based on the number of edits (insertions, deletions, substitutions) it takes to change one string into the other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### regular expressions\n",
    "\n",
    "a language for specifying text search strings\n",
    "\n",
    "**quick regex review:**\n",
    " \n",
    "disjunction\n",
    "- `[wW]oodchuck` - Woodchuck or woodchuck\n",
    "- `[abc]`  - ‘a’, ‘b’, or ‘c’\n",
    "- `gupp(y|ies)` - guppy or guppies\n",
    "\n",
    "range and `^` as *negation*\n",
    "- `[0-9]`  - a single digit 0-9\n",
    "- `[ ˆA-Z]` - not an upper case letter\n",
    "\n",
    "optional elements: `?`\n",
    "- `colou?r` - color or colour\n",
    "\n",
    "kleene star - zero or more occurrences of the immediately previous character or regular expression\n",
    "- `[ab]*` - aaaa, ababab, bbbb\n",
    "\n",
    "wildcard `.`\n",
    "- `beg.n`: begin, beg’n, begun\n",
    "\n",
    "anchors\n",
    "- `ˆThe box\\.$` - a line that contains only the phrase `The box`\n",
    "- /\\bthe\\b/ - `the` (but not the word other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### words\n",
    "\n",
    "**corpus** - a computer-readable collection of text or speech\n",
    "\n",
    "**utterance** - the spoken correlate of a sentence\n",
    "\n",
    "*I do uh main- mainly business data processing*\n",
    "\n",
    "- disfluencies occur in spoken sentences\n",
    "    - uh and um are called fillers\n",
    "    - sometimes these helpful because they may signal the restart of a clause or idea\n",
    "\n",
    "**lemma** - is a set of lexical forms having the same stem, the same major part-of-speech, and the same word sense\n",
    "- box, boxes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Normalization\n",
    "\n",
    "1. Tokenizing (segmenting) words\n",
    "2. Normalizing word formats\n",
    "3. Segmenting sentences\n",
    "\n",
    "unix example of tokenizing a text file\n",
    "\n",
    "`tr -sc ’A-Za-z’ ’\\n’ < sh.txt | tr A-Z a-z | sort | uniq -c | sort -n -r`\n",
    "\n",
    "- changes every sequence of nonalphabetic characters to a newline\n",
    "- \\-c option complements to non-alphabet\n",
    "- \\-s option squeezes all sequences into a single character\n",
    "- \\-n option sorts numerically rather than alphabetically\n",
    "- \\-r option means to sort in reverse order\n",
    "\n",
    "result:\n",
    "    \n",
    "    27378 the\n",
    "    26084 and\n",
    "    22538 i\n",
    "    19771 to\n",
    "    17481 of\n",
    "    14725 a\n",
    "    13826 you\n",
    "    12489 my\n",
    "    11318 that\n",
    "    11112 in\n",
    "    \n",
    "\n",
    "**function words** - articles, pronouns, prepositions, the most frequent corpora\n",
    "\n",
    "**named entity detection** - the task of detecting names, dates, and organizations \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**morpheme** - the smallest meaning-bearing unit of a language\n",
    "\n",
    "## tokenization\n",
    "What's the point of tokenization?\n",
    "\n",
    "- it’s helpful to have subword tokens to deal with unknown words\n",
    "- ML systems learn facts about words in a training corpus and then use that to make decisions about a test corpus\n",
    "-  if our training corpus contains, say the words low, and lowest, but not lower, but then the word \"lower\" appears in our test corpus, our system will be able to combine tokens from the training corpus to understand \"lower\"\n",
    "- most tokens are words, but some tokens are frequent morphemes or other subwords like -er, so that an unseen word can be represented by combining the parts\n",
    "\n",
    "\n",
    "### byte-pair encoding for tokenization\n",
    "\n",
    "based on a method for text compression, the intuition of the algorithm is to iteratively merge frequent pairs of characters\n",
    "\n",
    "algorithm:\n",
    "\n",
    ">1. Initialize vocabulary with the set of symbols equal to all characters seen plus a \"_\"\n",
    ">2. Represent each word in the corpus as a combination of the characters along with the special end of word token </w>.\n",
    ">3. Iteratively count character pairs in all tokens of the vocabulary.\n",
    ">4. Merge every occurrence of the most frequent pair, add the new character n-gram to the vocabulary.\n",
    ">5. Repeat step 3 and 4 until the desired number of merge operations are completed or the desired vocabulary size is achieved (which is a hyperparameter).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### my byte-pair encoding algorithm for tokenization\n",
    "- inputs: dictionary of words with frequencies of those words\n",
    "- K: number of iterations to run the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = '''\n",
    "Smoke the first 48 hours, grind 22 and sleep two hours\n",
    "Put 24s on the new Audi, white on white like baby powder\n",
    "Drop ya' boo off at Fulton County\n",
    "Might count it up and then re-count it\n",
    "Double cups like Tunechi, yeah\n",
    "Bust it down with these goonies, yeah\n",
    "Give no effs yeah, we don't give no effs, yeah\n",
    "Go fill my cup yeah, yo go fill my cup, yeah\n",
    "You heard that the slums made me, I'm cool with the Konvicts\n",
    "The coupe look like Akon, eff all that bum shit\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(corpus):\n",
    "    '''\n",
    "    Initialize vocabulary, add special char </w> to each word\n",
    "    ''' \n",
    "    chars = [\" \".join(word) + \" </w>\" for word in corpus.split()]\n",
    "    \n",
    "    # Count frequency of chars in corpus\n",
    "    vocab = Counter(chars)\n",
    "\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'t h e </w>': 4, 'y e a h </w>': 4, 'l i k e </w>': 3, 'i t </w>': 3, 'a n d </w>': 2, 'o n </w>': 2, 'w h i t e </w>': 2, 'w i t h </w>': 2, 'n o </w>': 2, 'y e a h , </w>': 2, 'f i l l </w>': 2, 'm y </w>': 2, 't h a t </w>': 2, 'S m o k e </w>': 1, 'f i r s t </w>': 1, '4 8 </w>': 1, 'h o u r s , </w>': 1, 'g r i n d </w>': 1, '2 2 </w>': 1, 's l e e p </w>': 1, 't w o </w>': 1, 'h o u r s </w>': 1, 'P u t </w>': 1, '2 4 s </w>': 1, 'n e w </w>': 1, 'A u d i , </w>': 1, 'b a b y </w>': 1, 'p o w d e r </w>': 1, 'D r o p </w>': 1, \"y a ' </w>\": 1, 'b o o </w>': 1, 'o f f </w>': 1, 'a t </w>': 1, 'F u l t o n </w>': 1, 'C o u n t y </w>': 1, 'M i g h t </w>': 1, 'c o u n t </w>': 1, 'u p </w>': 1, 't h e n </w>': 1, 'r e - c o u n t </w>': 1, 'D o u b l e </w>': 1, 'c u p s </w>': 1, 'T u n e c h i , </w>': 1, 'B u s t </w>': 1, 'd o w n </w>': 1, 't h e s e </w>': 1, 'g o o n i e s , </w>': 1, 'G i v e </w>': 1, 'e f f s </w>': 1, 'w e </w>': 1, \"d o n ' t </w>\": 1, 'g i v e </w>': 1, 'e f f s , </w>': 1, 'G o </w>': 1, 'c u p </w>': 1, 'y o </w>': 1, 'g o </w>': 1, 'c u p , </w>': 1, 'Y o u </w>': 1, 'h e a r d </w>': 1, 's l u m s </w>': 1, 'm a d e </w>': 1, 'm e , </w>': 1, \"I ' m </w>\": 1, 'c o o l </w>': 1, 'K o n v i c t s </w>': 1, 'T h e </w>': 1, 'c o u p e </w>': 1, 'l o o k </w>': 1, 'A k o n , </w>': 1, 'e f f </w>': 1, 'a l l </w>': 1, 'b u m </w>': 1, 's h i t </w>': 1})\n"
     ]
    }
   ],
   "source": [
    "vocab = build_vocab(corpus)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pair_counts(vocab):\n",
    "    '''\n",
    "    Get counts of the pairs of all consecutive symbols\n",
    "    '''\n",
    "\n",
    "    pair_counts = defaultdict(int)\n",
    "    for word, frequency in vocab.items():\n",
    "        chars = word.split()\n",
    "\n",
    "        for i in range(len(chars) - 1):\n",
    "            pair_counts[chars[i], chars[i + 1]] += frequency\n",
    "\n",
    "    return pair_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {('S', 'm'): 1, ('m', 'o'): 1, ('o', 'k'): 2, ('k', 'e'): 4, ('e', '</w>'): 18, ('t', 'h'): 10, ('h', 'e'): 8, ('f', 'i'): 3, ('i', 'r'): 1, ('r', 's'): 3, ('s', 't'): 2, ('t', '</w>'): 14, ('4', '8'): 1, ('8', '</w>'): 1, ('h', 'o'): 2, ('o', 'u'): 8, ('u', 'r'): 2, ('s', ','): 3, (',', '</w>'): 10, ('g', 'r'): 1, ('r', 'i'): 1, ('i', 'n'): 1, ('n', 'd'): 3, ('d', '</w>'): 4, ('2', '2'): 1, ('2', '</w>'): 1, ('a', 'n'): 2, ('s', 'l'): 2, ('l', 'e'): 2, ('e', 'e'): 1, ('e', 'p'): 1, ('p', '</w>'): 4, ('t', 'w'): 1, ('w', 'o'): 1, ('o', '</w>'): 7, ('s', '</w>'): 6, ('P', 'u'): 1, ('u', 't'): 1, ('2', '4'): 1, ('4', 's'): 1, ('o', 'n'): 7, ('n', '</w>'): 5, ('n', 'e'): 2, ('e', 'w'): 1, ('w', '</w>'): 1, ('A', 'u'): 1, ('u', 'd'): 1, ('d', 'i'): 1, ('i', ','): 2, ('w', 'h'): 2, ('h', 'i'): 4, ('i', 't'): 8, ('t', 'e'): 2, ('l', 'i'): 3, ('i', 'k'): 3, ('b', 'a'): 1, ('a', 'b'): 1, ('b', 'y'): 1, ('y', '</w>'): 4, ('p', 'o'): 1, ('o', 'w'): 2, ('w', 'd'): 1, ('d', 'e'): 2, ('e', 'r'): 1, ('r', '</w>'): 1, ('D', 'r'): 1, ('r', 'o'): 1, ('o', 'p'): 1, ('y', 'a'): 1, ('a', \"'\"): 1, (\"'\", '</w>'): 1, ('b', 'o'): 1, ('o', 'o'): 4, ('o', 'f'): 1, ('f', 'f'): 4, ('f', '</w>'): 2, ('a', 't'): 3, ('F', 'u'): 1, ('u', 'l'): 1, ('l', 't'): 1, ('t', 'o'): 1, ('C', 'o'): 1, ('u', 'n'): 4, ('n', 't'): 3, ('t', 'y'): 1, ('M', 'i'): 1, ('i', 'g'): 1, ('g', 'h'): 1, ('h', 't'): 1, ('c', 'o'): 4, ('u', 'p'): 5, ('e', 'n'): 1, ('r', 'e'): 1, ('e', '-'): 1, ('-', 'c'): 1, ('D', 'o'): 1, ('u', 'b'): 1, ('b', 'l'): 1, ('c', 'u'): 3, ('p', 's'): 1, ('T', 'u'): 1, ('e', 'c'): 1, ('c', 'h'): 1, ('y', 'e'): 6, ('e', 'a'): 7, ('a', 'h'): 6, ('h', '</w>'): 6, ('B', 'u'): 1, ('u', 's'): 1, ('d', 'o'): 2, ('w', 'n'): 1, ('w', 'i'): 2, ('e', 's'): 2, ('s', 'e'): 1, ('g', 'o'): 2, ('n', 'i'): 1, ('i', 'e'): 1, ('G', 'i'): 1, ('i', 'v'): 2, ('v', 'e'): 2, ('n', 'o'): 2, ('e', 'f'): 3, ('f', 's'): 2, ('h', ','): 2, ('w', 'e'): 1, ('n', \"'\"): 1, (\"'\", 't'): 1, ('g', 'i'): 1, ('G', 'o'): 1, ('i', 'l'): 2, ('l', 'l'): 3, ('l', '</w>'): 4, ('m', 'y'): 2, ('y', 'o'): 1, ('p', ','): 1, ('Y', 'o'): 1, ('u', '</w>'): 1, ('a', 'r'): 1, ('r', 'd'): 1, ('h', 'a'): 2, ('l', 'u'): 1, ('u', 'm'): 2, ('m', 's'): 1, ('m', 'a'): 1, ('a', 'd'): 1, ('m', 'e'): 1, ('e', ','): 1, ('I', \"'\"): 1, (\"'\", 'm'): 1, ('m', '</w>'): 2, ('o', 'l'): 1, ('K', 'o'): 1, ('n', 'v'): 1, ('v', 'i'): 1, ('i', 'c'): 1, ('c', 't'): 1, ('t', 's'): 1, ('T', 'h'): 1, ('p', 'e'): 1, ('l', 'o'): 1, ('k', '</w>'): 1, ('A', 'k'): 1, ('k', 'o'): 1, ('n', ','): 1, ('a', 'l'): 1, ('b', 'u'): 1, ('s', 'h'): 1})\n"
     ]
    }
   ],
   "source": [
    "pair_counts = get_pair_counts(vocab)\n",
    "print(pair_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('e', '</w>')\n"
     ]
    }
   ],
   "source": [
    "most_frequent = max(pair_counts, key=pair_counts.get)\n",
    "print(most_frequent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_most_frequent(pair: tuple, v_in: dict) -> dict:\n",
    "    '''\n",
    "    Merge all occurrences of the most frequent pair\n",
    "    '''\n",
    "    \n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    \n",
    "    for word in v_in:\n",
    "        # replace most frequent pair in all vocabulary\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "\n",
    "    return v_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'S m o k e</w>': 1, 't h e</w>': 4, 'f i r s t </w>': 1, '4 8 </w>': 1, 'h o u r s , </w>': 1, 'g r i n d </w>': 1, '2 2 </w>': 1, 'a n d </w>': 2, 's l e e p </w>': 1, 't w o </w>': 1, 'h o u r s </w>': 1, 'P u t </w>': 1, '2 4 s </w>': 1, 'o n </w>': 2, 'n e w </w>': 1, 'A u d i , </w>': 1, 'w h i t e</w>': 2, 'l i k e</w>': 3, 'b a b y </w>': 1, 'p o w d e r </w>': 1, 'D r o p </w>': 1, \"y a ' </w>\": 1, 'b o o </w>': 1, 'o f f </w>': 1, 'a t </w>': 1, 'F u l t o n </w>': 1, 'C o u n t y </w>': 1, 'M i g h t </w>': 1, 'c o u n t </w>': 1, 'i t </w>': 3, 'u p </w>': 1, 't h e n </w>': 1, 'r e - c o u n t </w>': 1, 'D o u b l e</w>': 1, 'c u p s </w>': 1, 'T u n e c h i , </w>': 1, 'y e a h </w>': 4, 'B u s t </w>': 1, 'd o w n </w>': 1, 'w i t h </w>': 2, 't h e s e</w>': 1, 'g o o n i e s , </w>': 1, 'G i v e</w>': 1, 'n o </w>': 2, 'e f f s </w>': 1, 'y e a h , </w>': 2, 'w e</w>': 1, \"d o n ' t </w>\": 1, 'g i v e</w>': 1, 'e f f s , </w>': 1, 'G o </w>': 1, 'f i l l </w>': 2, 'm y </w>': 2, 'c u p </w>': 1, 'y o </w>': 1, 'g o </w>': 1, 'c u p , </w>': 1, 'Y o u </w>': 1, 'h e a r d </w>': 1, 't h a t </w>': 2, 's l u m s </w>': 1, 'm a d e</w>': 1, 'm e , </w>': 1, \"I ' m </w>\": 1, 'c o o l </w>': 1, 'K o n v i c t s </w>': 1, 'T h e</w>': 1, 'c o u p e</w>': 1, 'l o o k </w>': 1, 'A k o n , </w>': 1, 'e f f </w>': 1, 'a l l </w>': 1, 'b u m </w>': 1, 's h i t </w>': 1}\n"
     ]
    }
   ],
   "source": [
    "vocab = merge_most_frequent(most_frequent, vocab)\n",
    "print(vocab)\n",
    "# you can see the 'e</w>'s being merged in the following vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Smoke</w>': 1, 'the</w>': 4, 'first</w>': 1, '48</w>': 1, 'hours,</w>': 1, 'grind</w>': 1, '22</w>': 1, 'and</w>': 2, 'sleep</w>': 1, 'two</w>': 1, 'hours</w>': 1, 'Put</w>': 1, '24s</w>': 1, 'on</w>': 2, 'new</w>': 1, 'Audi,</w>': 1, 'white</w>': 2, 'like</w>': 3, 'baby</w>': 1, 'powder</w>': 1, 'D r o p</w>': 1, \"y a ' </w>\": 1, 'b o o</w>': 1, 'o ff </w>': 1, 'at</w>': 1, 'F u l t on</w>': 1, 'C oun t y</w>': 1, 'M i g h t</w>': 1, 'count</w>': 1, 'it</w>': 3, 'u p</w>': 1, 'the n</w>': 1, 'r e - count</w>': 1, 'D ou b l e</w>': 1, 'cup s</w>': 1, 'T u ne c hi ,</w>': 1, 'yeah</w>': 4, 'B u st</w>': 1, 'd ow n</w>': 1, 'with</w>': 2, 'the s e</w>': 1, 'g o on i e s,</w>': 1, 'G ive</w>': 1, 'no</w>': 2, 'eff s</w>': 1, 'yeah,</w>': 2, 'w e</w>': 1, \"d on ' t</w>\": 1, 'g ive</w>': 1, 'eff s,</w>': 1, 'G o</w>': 1, 'fill</w>': 2, 'my</w>': 2, 'cu p</w>': 1, 'y o</w>': 1, 'g o</w>': 1, 'cup ,</w>': 1, 'Y ou </w>': 1, 'h ea r d</w>': 1, 'that</w>': 2, 'sl um s</w>': 1, 'm a d e</w>': 1, 'm e ,</w>': 1, \"I ' m </w>\": 1, 'c oo l</w>': 1, 'K on v i c t s</w>': 1, 'T h e</w>': 1, 'c ou p e</w>': 1, 'l oo k </w>': 1, 'A k on ,</w>': 1, 'eff </w>': 1, 'a ll</w>': 1, 'b um </w>': 1, 's hi t</w>': 1}\n"
     ]
    }
   ],
   "source": [
    "# now run this K more times\n",
    "num_merges = 50\n",
    "for i in range(num_merges):\n",
    "    pair_counts = get_pair_counts(vocab)\n",
    "\n",
    "    if not pair_counts:\n",
    "        break\n",
    "\n",
    "    most_frequent = max(pair_counts, key=pair_counts.get)\n",
    "    vocab = merge_most_frequent(most_frequent, vocab)\n",
    "\n",
    "    \n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References**\n",
    "1. [Speech and Language Processing - Chapter 2](https://web.stanford.edu/~jurafsky/slp3/2.pdf)\n",
    "2. [Byte Pair Encoding — The Dark Horse of Modern NLP](https://towardsdatascience.com/byte-pair-encoding-the-dark-horse-of-modern-nlp-eb36c7df4f10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
