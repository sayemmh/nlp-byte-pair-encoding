{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Expressions, Text Normalization, Edit Distance\n",
    "\n",
    "**text normalization** - converting text to a more convenient, standard form\n",
    "\n",
    "**lemmatization** - the task of determining that two words have the same root, despite their surface differences\n",
    "- sang, sung, and sings are forms of the verb sing\n",
    "- lemmatizer (a function) maps these words to their lemma, sing\n",
    "\n",
    "**sentence segmentation** - breaking up a text into individual sentences, using cues like periods or exclamation points\n",
    "\n",
    "**edit distance** - metric that measures how similar two strings are based on the number of edits (insertions, deletions, substitutions) it takes to change one string into the other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### regular expressions\n",
    "\n",
    "a language for specifying text search strings\n",
    "\n",
    "**quick regex review:**\n",
    " \n",
    "disjunction\n",
    "- `[wW]oodchuck` - Woodchuck or woodchuck\n",
    "- `[abc]`  - ‘a’, ‘b’, or ‘c’\n",
    "- `gupp(y|ies)` - guppy or guppies\n",
    "\n",
    "range and `^` as *negation*\n",
    "- `[0-9]`  - a single digit 0-9\n",
    "- `[ ˆA-Z]` - not an upper case letter\n",
    "\n",
    "optional elements: `?`\n",
    "- `colou?r` - color or colour\n",
    "\n",
    "kleene star - zero or more occurrences of the immediately previous character or regular expression\n",
    "- `[ab]*` - aaaa, ababab, bbbb\n",
    "\n",
    "wildcard `.`\n",
    "- `beg.n`: begin, beg’n, begun\n",
    "\n",
    "anchors\n",
    "- `ˆThe box\\.$` - a line that contains only the phrase `The box`\n",
    "- /\\bthe\\b/ - `the` (but not the word other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### words\n",
    "\n",
    "**corpus** - a computer-readable collection of text or speech\n",
    "\n",
    "**utterance** - the spoken correlate of a sentence\n",
    "\n",
    "*I do uh main- mainly business data processing*\n",
    "\n",
    "- disfluencies occur in spoken sentences\n",
    "    - uh and um are called fillers\n",
    "    - sometimes these helpful because they may signal the restart of a clause or idea\n",
    "\n",
    "**lemma** - is a set of lexical forms having the same stem, the same major part-of-speech, and the same word sense\n",
    "- box, boxes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Normalization\n",
    "\n",
    "1. Tokenizing (segmenting) words\n",
    "2. Normalizing word formats\n",
    "3. Segmenting sentences\n",
    "\n",
    "unix example of tokenizing a text file\n",
    "\n",
    "`tr -sc ’A-Za-z’ ’\\n’ < sh.txt | tr A-Z a-z | sort | uniq -c | sort -n -r`\n",
    "\n",
    "- changes every sequence of nonalphabetic characters to a newline\n",
    "- \\-c option complements to non-alphabet\n",
    "- \\-s option squeezes all sequences into a single character\n",
    "- \\-n option sorts numerically rather than alphabetically\n",
    "- \\-r option means to sort in reverse order\n",
    "\n",
    "result:\n",
    "```\n",
    "27378 the\n",
    "26084 and\n",
    "22538 i\n",
    "19771 to\n",
    "17481 of\n",
    "14725 a\n",
    "13826 you\n",
    "12489 my\n",
    "11318 that\n",
    "11112 in\n",
    "```\n",
    "\n",
    "**function words** - articles, pronouns, prepositions, the most frequent corpora\n",
    "\n",
    "**named entity detection** - the task of detecting names, dates, and organizations \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**morpheme** - the smallest meaning-bearing unit of a language\n",
    "\n",
    "ML systems learn facts about words in a training corpus and then use that to make decisions about a test corpus \n",
    "\n",
    "### byte-pair encoding for tokenization\n",
    "\n",
    "based on a method for text compression, the intuition of the algorithm is to iteratively merge frequent pairs of characters\n",
    "\n",
    "algorithm:\n",
    "1. initialize a \"vocabulary\" with the set of symbols equal to the set of characters plus a \"_\"\n",
    "2. represent each word in dictionary as a sequence of characters in the vocabulary\n",
    "3. count the number of symbol pairs in the current dictionary\n",
    "4. find the most frequent symbol pair\n",
    "5. add the merged symbol to our vocabulary\n",
    "6. merge that symbol pair across the dictionary\n",
    "7. repeat #3-#6 K times\n",
    "8. the resulting vocabulary will consist of the original set of characters plus k new symbols\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'low': ['l', 'o', 'w'], 'lowest': ['l', 'o', 'w', 'e', 's', 't'], 'newer': ['n', 'e', 'w', 'e', 'r'], 'wider': ['w', 'i', 'd', 'e', 'r'], 'new': ['n', 'e', 'w']}\n",
      "{'s', 'd', 't', 'o', 'i', 'r', '_', 'l', 'n', 'w', 'e'}\n"
     ]
    }
   ],
   "source": [
    "d = {\"low\": 5, \"lowest\": 2, \"newer\": 6, \"wider\": 3, \"new\": 2}\n",
    "\n",
    "def byte_pair_tokenize(D, K):\n",
    "    vocab = set()\n",
    "    vocab.add(\"_\")\n",
    "    merged = {}\n",
    "    for key, val in D.items():\n",
    "        vocab.update(list(key))\n",
    "        merged[key] = list(key)\n",
    "        \n",
    "    print(merged)    \n",
    "    print(vocab)\n",
    "    \n",
    "byte_pair_tokenize(d, 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
